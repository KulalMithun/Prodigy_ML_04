<h1>👋 Prodigy_ML_04 - Hand Gesture Recognition</h1>

<p>
  This is the <strong>4th machine learning project</strong> I completed as part of the <strong>Prodigy Infotech Virtual Internship</strong>. The goal of this project was to build a <strong>Hand Gesture Recognition System</strong> using machine learning techniques.
</p>

<h2>🎯 Project Aim</h2>
<ul>
  <li>Recognize hand gestures using deep learning.</li>
  <li>Achieve high model accuracy on gesture classification.</li>
  <li>Integrate live prediction using webcam feed with OpenCV.</li>
</ul>

<h2>📁 Dataset</h2>
<p>
  The dataset used in this project is the <a href="https://www.kaggle.com/datasets/gti-upm/leapgestrecog" target="_blank">LeapGestRecog</a> dataset, which contains near-infrared images of hand gestures captured using a Leap Motion sensor.
</p>

<h2>🛠️ Tools and Technologies</h2>
<ul>
  <li><strong>Python</strong></li>
  <li><strong>Keras</strong> for model building and training</li>
  <li><strong>OpenCV</strong> for real-time hand gesture prediction</li>
</ul>

<h2>📊 Performance</h2>
<p>
  The model was trained using Keras and achieved a <strong>99% validation accuracy</strong>, showing strong performance in gesture classification.
</p>

<h2>🎥 Live Gesture Prediction</h2>
<p>
  The project includes a live prediction module that uses your system's webcam to detect and classify hand gestures in real-time. OpenCV is integrated with the trained model to make this possible.
</p>

<p><strong>To exit live prediction mode, press <code>q</code> on your keyboard.</strong></p>

<h2>💡 What I Learned</h2>
<ul>
  <li>Hands-on experience in building and evaluating machine learning models</li>
  <li>Real-time computer vision application using OpenCV</li>
  <li>Understanding how to preprocess image data for CNN models</li>
</ul>

<h2>📌 Conclusion</h2>
<p>
  This project gave me practical exposure to machine learning workflows, from data preparation to model deployment. It strengthened my understanding of both deep learning and computer vision.
</p>
